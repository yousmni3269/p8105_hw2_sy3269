p8105_hw2_sy3269.Rmd
================

Required packages (e.g.`tidyverse` and `readxl`) are installed.

# Problem 1

### NYC metro

``` r
metro_df = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv",  na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  select(line:route11, entry, vending, entrance_type, ada) |>
  mutate(
    entry = case_match(
      entry, 
      "YES" ~ TRUE,
      "NO" ~ FALSE
    ))
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
metro_df
```

    ## # A tibble: 1,868 × 19
    ##    line     station_name station_latitude station_longitude route1 route2 route3
    ##    <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ##  1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ##  3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  4 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  5 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ##  6 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  7 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  8 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ##  9 4 Avenue 45th St                  40.6             -74.0 R      <NA>   <NA>  
    ## 10 4 Avenue 53rd St                  40.6             -74.0 R      <NA>   <NA>  
    ## # ℹ 1,858 more rows
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>, entry <lgl>,
    ## #   vending <chr>, entrance_type <chr>, ada <lgl>

The NYC subway data was imported and cleaned using a
`janitor::clean_names` function. Then the entry variable was converted
from a character variable to a logical one using `case_match`. Then only
the required variables were listed out to remove unnecessary information
from the data set.

The NYC metro data set contains line, station name, station location
(latitude and longitude), routes served, entry and vending availability,
entrance type, and ada compliance information.

The dimension of the data set is 1868 x 19. The data set is in human
readable form but not yet tidy.

``` r
unique_stn = distinct(metro_df, station_name, line, ada)

ada_stn = filter(unique_stn, ada == TRUE)

nv_stn = filter(metro_df, vending == "NO")

nv_entry_stn = filter(nv_stn, entry == TRUE)
```

There are 465 distinct stations and there are 84 ADA compliant stations.

69 stations out of 183 stations without vending allow entrance. The
proportion is about 2.6521739.

``` r
new_metro_df = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |> 
  mutate(
    route7 = as.character(route7),
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
  ) |> 
  select(line, station_name, route1:route11, entry, vending, entrance_type, ada) |>
  pivot_longer( 
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  )
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The data set was reformatted so that `route number` and `route name` are
distinct variables.

``` r
stn_for_A = 
  new_metro_df |>
  filter(route_name == "A") |>
  select(station_name, line) |>
  distinct()

stn_for_A_ada = 
  new_metro_df |>
  filter(route_name == "A", ada ==TRUE) |>
  select(station_name, line) |>
  distinct()
```

There are 60 stations that serve the A train and 17 of these stations
are ADA compliant.

# Problem 2

### Mr. Trash Wheel

``` r
mr_tw_df = 
  read_excel("./202409 Trash Wheel Collection Data.xlsx", 
    sheet = "Mr. Trash Wheel",
    skip = 1, 
    n_max = 651,
    na = c("NA", "", "."),
    .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(
    sports_balls = 
      sports_balls |>
      round() |>
      as.integer(),
    year = as.integer(year), 
    trash_wheel_type = "mr_trash_wheel") |>
  arrange(year, month) 
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
mr_tw_df = 
  mr_tw_df |>
  select(-starts_with("x"))

head(mr_tw_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month     year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>    <int> <dttm>                    <dbl>              <dbl>
    ## 1       25 August    2014 2014-08-04 00:00:00        4.39                 16
    ## 2       26 August    2014 2014-08-04 00:00:00        5.33                 17
    ## 3       27 August    2014 2014-08-13 00:00:00        3.58                 20
    ## 4       28 August    2014 2014-08-13 00:00:00        3.1                  17
    ## 5       29 August    2014 2014-08-19 00:00:00        1.77                 10
    ## 6       42 December  2014 2014-12-01 00:00:00        1.81                 17
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>,
    ## #   trash_wheel_type <chr>

``` r
tail(mr_tw_df)
```

    ## # A tibble: 6 × 15
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <int> <dttm>                    <dbl>              <dbl>
    ## 1      641 March  2024 2024-03-29 00:00:00        3.34                 15
    ## 2      645 May    2024 2024-05-02 00:00:00        4.9                  15
    ## 3      646 May    2024 2024-05-10 00:00:00        3.68                 15
    ## 4      647 May    2024 2024-05-10 00:00:00        4.7                  15
    ## 5      648 May    2024 2024-05-30 00:00:00        4.13                 15
    ## 6      649 May    2024 2024-05-30 00:00:00        3.34                 15
    ## # ℹ 9 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, sports_balls <int>, homes_powered <dbl>,
    ## #   trash_wheel_type <chr>

Mr. Trash Wheel excel sheet from 2024 data set was imported as
`mr_tw_df`and was cleaned. Any unnecessary data, such as non-dumpster
specific data, was removed and `trash_wheel_type` variable was added to
the data frame so that I can keep track of which trash wheel data is
which when I later bind this data frame with other trash wheel data. The
data was arranged in the order of time.

The data was checked once again and any unnecessary empty columns
created after importing was deleted. Head and tail of the cleaned data
was checked.

``` r
prof_tw_df = 
  read_excel("./202409 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", 
  skip = 1, 
  n_max = 118, 
  na = c("NA", "", "."),
  .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(trash_wheel_type = "professor_trash_wheel") |>
  arrange(year, month)

gwyn_tw_df = read_excel(
  "./202409 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", 
  skip = 1, 
  n_max = 263,
  na = c("NA", "", "."),
  .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(trash_wheel_type = "gwynnda_trash_wheel") |>
  arrange(year, month) 

head(prof_tw_df)
```

    ## # A tibble: 6 × 14
    ##   dumpster month     year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>    <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        7 April     2017 2017-04-01 00:00:00        1.82                 15
    ## 2        8 April     2017 2017-04-20 00:00:00        2.37                 15
    ## 3       15 August    2017 2017-08-04 00:00:00        2.93                 15
    ## 4       16 August    2017 2017-08-31 00:00:00        1.21                 15
    ## 5        3 February  2017 2017-02-26 00:00:00        2.32                 18
    ## 6        4 February  2017 2017-02-26 00:00:00        3.72                 15
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel_type <chr>

``` r
tail(prof_tw_df)
```

    ## # A tibble: 6 × 14
    ##   dumpster month      year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>     <dbl> <dttm>                    <dbl>              <dbl>
    ## 1      111 September  2023 2023-09-06 00:00:00        1.71                 10
    ## 2      112 September  2023 2023-09-26 00:00:00        2.43                 15
    ## 3      117 April      2024 2024-04-16 00:00:00        3                    15
    ## 4      115 January    2024 2024-01-08 00:00:00        2.2                  15
    ## 5      116 March      2024 2024-03-14 00:00:00        3.75                 15
    ## 6      118 May        2024 2024-05-30 00:00:00        2.48                 15
    ## # ℹ 8 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, glass_bottles <dbl>, plastic_bags <dbl>,
    ## #   wrappers <dbl>, homes_powered <dbl>, trash_wheel_type <chr>

``` r
head(gwyn_tw_df)
```

    ## # A tibble: 6 × 13
    ##   dumpster month   year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr>  <dbl> <dttm>                    <dbl>              <dbl>
    ## 1        6 August  2021 2021-08-11 00:00:00        2.06                 15
    ## 2        7 August  2021 2021-08-14 00:00:00        1.9                  15
    ## 3        8 August  2021 2021-08-16 00:00:00        2.16                 15
    ## 4        9 August  2021 2021-08-16 00:00:00        2.6                  15
    ## 5       10 August  2021 2021-08-17 00:00:00        3.21                 15
    ## 6       11 August  2021 2021-08-17 00:00:00        2.44                 15
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel_type <chr>

``` r
tail(gwyn_tw_df)
```

    ## # A tibble: 6 × 13
    ##   dumpster month  year date                weight_tons volume_cubic_yards
    ##      <dbl> <chr> <dbl> <dttm>                    <dbl>              <dbl>
    ## 1      255 May    2024 2024-05-14 00:00:00        3.27                 15
    ## 2      256 May    2024 2024-05-29 00:00:00        2.72                 15
    ## 3      257 May    2024 2024-05-29 00:00:00        3                    15
    ## 4      258 May    2024 2024-05-29 00:00:00        3.78                 15
    ## 5      259 May    2024 2024-05-30 00:00:00        3.35                 15
    ## 6      260 May    2024 2024-05-31 00:00:00        3.55                 15
    ## # ℹ 7 more variables: plastic_bottles <dbl>, polystyrene <dbl>,
    ## #   cigarette_butts <dbl>, plastic_bags <dbl>, wrappers <dbl>,
    ## #   homes_powered <dbl>, trash_wheel_type <chr>

Professor Trash Wheel and Gwynnda Trash Wheel excel sheets were also
imported and cleaned so that I have only necessary dumpster-specific
data. Similar to Mr. Trash Wheel data, any variable required to keep
track of trash wheel type when all data sets are binded together was
added to both Professor and Gwynnda data sets.

Head and tail of each cleaned data set was checked.

It was noticed that there are two observations from dumpster 21 in
Gwynnada Trash Wheel data set.

``` r
comb_tw_df = bind_rows(prof_tw_df, gwyn_tw_df, mr_tw_df) 

trash_types = 
  select(comb_tw_df, plastic_bottles:sports_balls) |>
  names()

wgt_prof_tw_df = 
  select(prof_tw_df, weight_tons) |>
  sum()

wgt_gwyn_tw_df = select(gwyn_tw_df, weight_tons) |>
  sum()

wgt_mr_tw_df = select(mr_tw_df, weight_tons) |>
  sum()

cb_comb_tw_df = 
  comb_tw_df |>
  select(cigarette_butts) |>
  sum()
  
cb_gwyn_tw_df = 
  gwyn_tw_df |>
  filter(year == 2022, month == "June") |>
  select(cigarette_butts) |>
  sum()
```

Professor Trash Wheel, Gwynnda Trash Wheel and Mr. Trash Wheel data were
all combined into one data set named `comb_tw_df`. In the merged data
set, total 1032 observations were found, where 118 observations came
from Professor Trash Wheel, 263 from Gwynnda Trash Wheel, and 651 from
Mr. Trash Wheel.

The observations are collected from August 2014 to May 2024 by Mr. Trash
Wheel; April 2017 to May 2024, Professor Trash Wheel; August 2021 to May
2024, Gwynnda Trash Wheel.

There are 9 types of trash collected by three different Trash Wheels and
they are as follows: plastic_bottles, polystyrene, cigarette_butts,
glass_bottles, plastic_bags, wrappers, homes_powered, trash_wheel_type,
sports_balls.

The total weight of trash collected by Professor Trash Wheel is 246.74
tons whereas Gwynnda Trash Wheel and Mr. Trash Wheel collected 797.55
tons and 2091.18 tons, respectively.

The total number of cigarette butts collected by three Trash Wheels are
NA and Gwynnda collected 1.812^{4} cigarette butts in June of 2022.

# Problem 3

### Great British Bake Off

``` r
bakers_df = 
  read_csv("./gbb_datasets/bakers.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  separate(
    baker_name, into = c("baker", "baker_last_name"), sep = " "
  ) |>
  arrange(series) |>
  relocate(series, baker)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = 
  read_csv("./gbb_datasets/bakes.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  mutate(baker = gsub("\"", "", baker)) 
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
results_df = 
  read_csv("./gbb_datasets/results.csv", na = c("NA", "", "."), skip = 2) |>
  janitor::clean_names() |>
  mutate(
    baker = replace(baker, baker == "Joanne", "Jo")
  )
```

    ## Rows: 1136 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (2): baker, result
    ## dbl (3): series, episode, technical
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_results_df = 
  full_join(bakes_df, results_df, by = c("series", "episode", "baker")) |>
  pivot_wider(
    names_from = episode,
    values_from = c(signature_bake, show_stopper, technical, result)
  )

gbboff_df = 
  full_join(bakers_df, bakes_results_df, by = c("series", "baker"))

bakers_check = anti_join(bakers_df, bakes_results_df) 
```

    ## Joining with `by = join_by(series, baker)`

First, I imported `bakers.csv` file and cleaned it using
`janitor::clean_names`. Then, I separated the `name` variable into
`baker` and `baker_last_name` for later binding as other files
(e.g.`bakes.csv` and `results.csv`) did not have last name information
under `baker` variable. Next, I arranged the data in the ascending
series order and relocated the data set to have series and first name as
first two columns.

Similarly, I imported and cleaned `bakes.csv` and `results.csv` files.
For `bakes.csv`, I eliminated unnecessary `""` marks in a value under
`baker` variable (e.g. Jo was input as “Jo”). For `results.csv`, I
changed a name of a baker in Season 2 Episode 1 from Joanne to Jo so
that this value aligns with that of the other data frames. This person
is input as Jo under baker information in other csv files.

Next, I combined the latter two data sets into one data frame named
`bakes_results_df` and reformatted the data using `pivot_wider` function
for further binding with `bakers_df` data set. In this new data frame,
there are ten columns, corresponding to ten episodes, for each
*signature_baker*, *show_stopper*, *technical*, and *result* information
in ascending episode number.

`bakers_df` and `bakes_results_df` were merged using `full_join`
function to retain all information, and the data frame was named as
`gbboff_df`.

`anti_join` was used to see if the merge was performed correctly. It was
confirmed that the data sets were merged successfully and correctly as
an empty tibble was generated using `anti_join`.

``` r
write.csv(gbboff_df, "./gbb_datasets/gbboff_merged_file.csv")
```

The merged data frame that retains all information from three data files
was exported as `.csv` file and was saved to the same directory as other
Great British Bake Off files.

``` r
gbboff_winner = 
  gbboff_df |>
  select(series, baker, result_1:result_10) |>
  pivot_longer(
    cols = result_1:result_10, 
    names_to = "episode", 
    values_to = "result"
  ) |> 
  filter(result == "STAR BAKER" | result == "WINNER", series > 4) |>
  mutate(
    episode = case_match(
      episode,
      "result_1" ~ "episode_1",
      "result_2" ~ "episode_2",
      "result_3" ~ "episode_3",
      "result_4" ~ "episode_4",
      "result_5" ~ "episode_5",
      "result_6" ~ "episode_6",
      "result_7" ~ "episode_7",
      "result_8" ~ "episode_8",
      "result_9" ~ "episode_9",
      "result_10" ~ "episode_10")
  )

clean_gbboff_winner = 
  gbboff_winner |>
  mutate(
    result = case_match(
      result, 
      "STAR BAKER" ~ "STAR BAKER",
      "WINNER" ~ "STAR BAKER")
  ) |>
  pivot_wider(
    names_from = episode, 
    values_from = baker
  ) |>
  select(series, episode_1, episode_2, episode_3, episode_4, episode_5, episode_6, episode_7, episode_8, episode_9, episode_10) |>
  knitr::kable()

clean_gbboff_winner
```

| series | episode_1 | episode_2 | episode_3 | episode_4 | episode_5 | episode_6 | episode_7 | episode_8 | episode_9 | episode_10 |
|-------:|:----------|:----------|:----------|:----------|:----------|:----------|:----------|:----------|:----------|:-----------|
|      5 | Nancy     | Richard   | Luis      | Richard   | Kate      | Chetna    | Richard   | Richard   | Richard   | Nancy      |
|      6 | Marie     | Ian       | Ian       | Ian       | Nadiya    | Mat       | Tamal     | Nadiya    | Nadiya    | Nadiya     |
|      7 | Jane      | Candice   | Tom       | Benjamina | Candice   | Tom       | Andrew    | Candice   | Andrew    | Candice    |
|      8 | Steven    | Steven    | Julia     | Kate      | Sophie    | Liam      | Steven    | Stacey    | Sophie    | Sophie     |
|      9 | Manon     | Rahul     | Rahul     | Dan       | Kim-Joy   | Briony    | Kim-Joy   | Ruby      | Ruby      | Rahul      |
|     10 | Michelle  | Alice     | Michael   | Steph     | Steph     | Steph     | Henry     | Steph     | Alice     | David      |

From the merged data set, only the series, baker name and results of
each episode were selected to create a new data frame named
`gbboff_winner`. Then, `pivit_longer` function was used to list out all
star bakers or winners of all episodes from season 5 to season 10. Some
of the values were altered for clarification.

Once the required information was organized as a long list, new data
frame `clean_gbboff_winner` was made based on it to make a more
human-readable data set. First, value `WINNER` under `result` variable
was changed to `STAR BAKER` so that all results can be summarized into
values when `pivot_wider` is applied. Then, the data was organized so
that each row would correspond to a season and each column to an
episode. The names of star bakers or winners are shown as values in the
table. To display the data as reader-friendly table, `knitr::kable` was
used.

From the table, it is surprising to see some of the bakers who were
*star bakers* multiple times in one season, meaning that they were
multiple episodes, actually did not win become a *winner* of the season.
For instance, Richard became the *star baker* five times in Season 5,
and Steph was a four time *star baker* in Season 10, but neither of them
were the winner of their season.

In fact, for season 10, it is surprising to see David who had never been
a *star baker* throughout the season won at the end.

``` r
viewers_df = read_csv("gbb_datasets/viewers.csv", na = c("NA", "", ".")) |>
  janitor::clean_names()
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
head(viewers_df, n = 10)
```

    ## # A tibble: 10 × 11
    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ##      <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>    <dbl>
    ##  1       1     2.24     3.1      3.85     6.6      8.51     11.6     13.6
    ##  2       2     3        3.53     4.6      6.65     8.79     11.6     13.4
    ##  3       3     3        3.82     4.53     7.17     9.28     12.0     13.0
    ##  4       4     2.6      3.6      4.71     6.82    10.2      12.4     13.3
    ##  5       5     3.03     3.83     4.61     6.95     9.95     12.4     13.1
    ##  6       6     2.75     4.25     4.82     7.32    10.1      12       13.1
    ##  7       7    NA        4.42     5.1      7.76    10.3      12.4     13.4
    ##  8       8    NA        5.06     5.35     7.41     9.02     11.1     13.3
    ##  9       9    NA       NA        5.7      7.41    10.7      12.6     13.4
    ## 10      10    NA       NA        6.74     9.45    13.5      15.0     15.9
    ## # ℹ 3 more variables: series_8 <dbl>, series_9 <dbl>, series_10 <dbl>

``` r
avg_viewers_s1 = 
  select(viewers_df, series_1) |> 
  drop_na() |>
  pull() |>
  mean()

avg_viewers_s5 = 
  select(viewers_df, series_5) |>
  pull() |>
  mean()
```

The viewership data in `viewers.csv` was imported and cleaned. The first
10 rows of the dataset is displayed using `head` function. The average
viewership in Season 1 is 2.77 and in Season 5 is 10.0393.
