---
title: "p8105_hw2_sy3269.Rmd"
output: github_document 
---

```{r setup, echo = FALSE, message = FALSE}
library(tidyverse)
library(readxl)
```

Required packages (e.g.`tidyverse` and `readxl`) are installed. 


# Problem 1
### NYC metro 

```{r metro_data}
metro_df = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv",  na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  select(line:route11, entry, vending, entrance_type, ada) |>
  mutate(
    entry = case_match(
      entry, 
      "YES" ~ TRUE,
      "NO" ~ FALSE
    ))

metro_df
```

The NYC subway data was imported and cleaned using a `janitor::clean_names` function. Then the entry variable was converted from a character variable to a logical one using `case_match`. Then only the required variables were listed out to remove unnecessary information from the data set. 

The NYC metro data set contains line, station name, station location (latitude and longitude), routes served, entry and vending availability, entrance type, and ada compliance information. 

The dimension of the data set is `r nrow(metro_df)` x `r ncol(metro_df)`. The data set is in human readable form but not yet tidy.  

```{r metro_questions}
unique_stn = distinct(metro_df, station_name, line, ada)

ada_stn = filter(unique_stn, ada == TRUE)

nv_stn = filter(metro_df, vending == "NO")

nv_entry_stn = filter(nv_stn, entry == TRUE)
```

There are `r count(unique_stn)` distinct stations and there are `r count(ada_stn)` ADA compliant stations. 

`r count(nv_entry_stn)` stations out of `r count(nv_stn)` stations without vending allow entrance. The proportion is about `r count(nv_stn)/count(nv_entry_stn)`. 

```{r, reformatting}
new_metro_df = 
  read_csv("./NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |> 
  mutate(
    route7 = as.character(route7),
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
  ) |> 
  select(line, station_name, route1:route11, entry, vending, entrance_type, ada) |>
  pivot_longer( 
    cols = route1:route11,
    names_to = "route_number",
    values_to = "route_name",
    names_prefix = "route"
  )
```

The data set was reformatted so that `route number` and `route name` are distinct variables. 

```{r, station_for_A}
stn_for_A = distinct(new_metro_df, route_name == "A", ada) 

stn_for_A_ada = filter(stn_for_A, ada == TRUE)
```

There are `r count(stn_for_A)` stations that serve the A train and `r count(stn_for_A_ada)` of these stations are ADA compliant. 



# Problem 2
### Mr. Trash Wheel

```{r, mr_trash_wheel}
mr_tw_df = 
  read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Mr. Trash Wheel",
  range = cell_cols("A:N"),
  skip = 1, 
  n_max = 584,
  na = c("NA", "", "."),
  .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(
    sports_balls = 
      sports_balls |>
      round() |>
      as.integer(),
    year = as.integer(year), 
    trash_wheel_type = "mr_trash_wheel") |>
  arrange(year, month) 

head(mr_tw_df)
tail(mr_tw_df)
```

Mr. Trash Wheel excel sheet was imported as `mr_tw_df`and was cleaned. Any unnecessary data, such as non-dumpster specific data or figure, was removed and required columns were specified for the import. Also, `trash_wheel_type` variable was added to the data frame so that I can keep track of which trash wheel data is which when I bind this data frame with other trash wheel data. Head and tail of the cleaned data was checked. 


``` {r prof_gwyn}
prof_tw_df = 
  read_excel("./202309 Trash Wheel Collection Data.xlsx", sheet = "Professor Trash Wheel", 
  range = cell_cols("A:M"),
  skip = 1, 
  n_max = 106, 
  na = c("NA", "", "."),
  .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(trash_wheel_type = "professor_trash_wheel") |>
  arrange(year, month)

gwyn_tw_df = read_excel(
  "./202309 Trash Wheel Collection Data.xlsx", sheet = "Gwynnda Trash Wheel", 
  range = cell_cols("A:L"),
  skip = 1, 
  n_max = 154,
  na = c("NA", "", "."),
  .name_repair = "unique") |>
  janitor::clean_names() |>
  mutate(trash_wheel_type = "gwynnda_trash_wheel") |>
  arrange(year, month)

head(prof_tw_df)
tail(prof_tw_df)
head(gwyn_tw_df)
tail(gwyn_tw_df)
```

Professor Trash Wheel and Gwynnda Trash Wheel excel sheets were also imported and cleaned so that I have only necessary dumpster-specific data. Similar to Mr. Trash Wheel data, any variable required to keep track of trash wheel type when all data sets are binded together was added to both Professor and Gwynnda data sets.

Head and tail of each cleaned data set was checked. 


```{r three_summary}
comb_tw_df = bind_rows(prof_tw_df, gwyn_tw_df, mr_tw_df) 

trash_types = 
  select(comb_tw_df, plastic_bottles:sports_balls) |>
  names()

wgt_prof_tw_df = 
  select(prof_tw_df, weight_tons) |>
  sum()

wgt_gwyn_tw_df = select(gwyn_tw_df, weight_tons) |>
  sum()

wgt_mr_tw_df = select(mr_tw_df, weight_tons) |>
  sum()

cb_comb_tw_df = 
  comb_tw_df |>
  select(cigarette_butts) |>
  sum()
  
cb_gwyn_tw_df = 
  gwyn_tw_df |>
  filter(year == 2022, month == "June") |>
  select(cigarette_butts) |>
  sum()
```

Professor Trash Wheel, Gwynnda Trash Wheel and Mr. Trash Wheel data were all combined into one data set named `comb_tw_df`. In the merged data set, total `r nrow(comb_tw_df)` observations were found, where `r nrow(prof_tw_df)` observations came from Professor Trash Wheel, `r nrow(gwyn_tw_df)` from Gwynnda Trash Wheel, and `r nrow(mr_tw_df)` from Mr. Trash Wheel. 

The observations are collected from May 2014 to June 2023 by Mr. Trash Wheel; 2017 April to June 2023, Professor Trash Wheel; August 2021 to May 2023, Gwynnda Trash Wheel. 

There are `r length(trash_types)` types of trash collected by three different Trash Wheels and they are as follows: `r trash_types`. 

The total weight of trash collected by Professor Trash Wheel is `r wgt_prof_tw_df` tons whereas Gwynnda Trash Wheel and Mr. Trash Wheel collected `r wgt_gwyn_tw_df` tons and `r wgt_mr_tw_df` tons, respectively. 

The total number of cigarette butts collected by three Trash Wheels are `r cb_comb_tw_df` and Gwynnda collected `r cb_gwyn_tw_df` cigarette butts in June of 2022. 


# Problem 3
### Great British Bake Off

```{r gbboff_first}
bakers_df = 
  read_csv("./gbb_datasets/bakers.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  separate(
    baker_name, into = c("baker", "baker_last_name"), sep = " "
  ) |>
  arrange(series) |>
  relocate(series, baker)
      
bakes_df = 
  read_csv("./gbb_datasets/bakes.csv", na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  mutate(baker = gsub("\"", "", baker)) 

results_df = 
  read_csv("./gbb_datasets/results.csv", na = c("NA", "", "."), skip = 2) |>
  janitor::clean_names() |>
  mutate(
    baker = replace(baker, baker == "Joanne", "Jo")
  )
  
bakes_results_df = 
  full_join(bakes_df, results_df, by = c("series", "episode", "baker")) |>
  pivot_wider(
    names_from = episode,
    values_from = c(signature_bake, show_stopper, technical, result)
  )

gbboff_df = 
  full_join(bakers_df, bakes_results_df, by = c("series", "baker"))

bakers_check = anti_join(bakers_df, bakes_results_df) 
```

First, I imported `bakers.csv` file and cleaned it using `janitor::clean_names`. Then, I separated the `name` variable into `baker` and `baker_last_name` for later binding as other files (e.g.`bakes.csv` and `results.csv`) did not have last name information under `baker` variable. Next, I arranged the data in the ascending series order and relocated the data set to have series and first name as first two columns.

Similarly, I imported and cleaned `bakes.csv` and `results.csv` files. For `bakes.csv`, I eliminated unnecessary `""` marks in a value under `baker` variable (e.g. Jo was input as "Jo"). For `results.csv`, I changed a name of a baker in Season 2 Episode 1 from Joanne to Jo so that this value aligns with that of the other data frames. This person is input as Jo under baker information in other csv files. 

Next, I combined the latter two data sets into one data frame named `bakes_results_df` and reformatted the data using `pivot_wider` function for further binding with `bakers_df` data set. In this new data frame, there are ten columns, corresponding to ten episodes, for each *signature_baker*, *show_stopper*, *technical*, and *result* information in ascending episode number. 

`bakers_df` and `bakes_results_df` were merged using `full_join` function to retain all information, and the data frame was named as `gbboff_df`. 

`anti_join` was used to see if the merge was performed correctly. It was confirmed that the data sets were merged successfully and correctly as an empty tibble was generated using `anti_join`. 


```{r gbboff_export}
write.csv(gbboff_df, "./gbb_datasets/gbboff_merged_file.csv")
```

The merged data frame that retains all information from three data files was exported as `.csv` file and was saved to the same directory as other Great British Bake Off files. 

```{r gbboff_table}
gbboff_winner = 
  gbboff_df |>
  select(series, baker, result_1:result_10) |>
  pivot_longer(
    cols = result_1:result_10, 
    names_to = "episode", 
    values_to = "result"
  ) |> 
  filter(result == "STAR BAKER" | result == "WINNER", series > 4) |>
  mutate(
    episode = case_match(
      episode,
      "result_1" ~ "episode_1",
      "result_2" ~ "episode_2",
      "result_3" ~ "episode_3",
      "result_4" ~ "episode_4",
      "result_5" ~ "episode_5",
      "result_6" ~ "episode_6",
      "result_7" ~ "episode_7",
      "result_8" ~ "episode_8",
      "result_9" ~ "episode_9",
      "result_10" ~ "episode_10")
  )

clean_gbboff_winner = 
  gbboff_winner |>
  mutate(
    result = case_match(
      result, 
      "STAR BAKER" ~ "STAR BAKER",
      "WINNER" ~ "STAR BAKER")
  ) |>
  pivot_wider(
    names_from = episode, 
    values_from = baker
  ) |>
  select(series, episode_1, episode_2, episode_3, episode_4, episode_5, episode_6, episode_7, episode_8, episode_9, episode_10) |>
  knitr::kable()

clean_gbboff_winner
```

From the merged data set, only the series, baker name and results of each episode were selected to create a new data frame named `gbboff_winner`. Then, `pivit_longer` function was used to list out all star bakers or winners of all episodes from season 5 to season 10. Some of the values were altered for clarification.

Once the required information was organized as a long list, new data frame `clean_gbboff_winner` was made based on it to make a more human-readable data set. First, value `WINNER` under `result` variable was changed to `STAR BAKER` so that all results can be summarized into values when `pivot_wider` is applied. Then, the data was organized so that each row would correspond to a season and each column to an episode. The names of star bakers or winners are shown as values in the table. To display the data as reader-friendly table, `knitr::kable` was used. 

From the table, it is surprising to see some of the bakers who were *star bakers* multiple times in one season, meaning that they were multiple episodes, actually did not win become a *winner* of the season. For instance, Richard became the *star baker* five times in Season 5, and Steph was a four time *star baker* in Season 10, but neither of them were the winner of their season. 

In fact, for season 10, it is surprising to see David who had never been a *star baker* throughout the season won at the end. 


```{r gbboff_viewer}
viewers_df = read_csv("gbb_datasets/viewers.csv", na = c("NA", "", ".")) |>
  janitor::clean_names()

head(viewers_df, n = 10)

avg_viewers_s1 = 
  select(viewers_df, series_1) |> 
  drop_na() |>
  pull() |>
  mean()

avg_viewers_s5 = 
  select(viewers_df, series_5) |>
  pull() |>
  mean()
```

The viewership data in `viewers.csv` was imported and cleaned. The first 10 rows of the dataset is displayed using `head` function. The average viewership in Season 1 is `r avg_viewers_s1` and in Season 5 is `r avg_viewers_s5`. 

